{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afe7d95e",
   "metadata": {},
   "source": [
    "# Generative Models with AE, VAE, and GANs\n",
    "This notebook demonstrates the construction, training, and comparison of three generative models: Autoencoder (AE), Variational Autoencoder (VAE), and Generative Adversarial Network (GAN). We will use the Fashion-MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2829c63-ad1a-49e2-a680-f735e8f8a179",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3540d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, LeakyReLU, Dropout, BatchNormalization, Lambda\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed52ec6-ecbf-4c70-9a5d-b0c58a56d225",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data (Fashion-MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38184ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "img_shape = x_train.shape[1:]\n",
    "latent_dim = 64 # Latent dimension for AE and VAE\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Image shape: {img_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d8ac9-d519-4669-873c-05ef00573fc9",
   "metadata": {},
   "source": [
    "### Display some training images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad43b4-cf52-489f-a0d8-135588b70129",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 2))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(1, 5, i + 1)\n",
    "    plt.imshow(x_train[i].reshape(28, 28), cmap='gray')\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.suptitle(\"Sample Training Images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ec1ef",
   "metadata": {},
   "source": [
    "## 3. Autoencoder (AE)\n",
    "An Autoencoder learns to compress data (encoding) and then reconstruct it (decoding)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca89458-4d58-4975-be6a-65a95de5486e",
   "metadata": {},
   "source": [
    "### 3.1. AE Model Definition\n",
    "\n",
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd050311",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.squeeze(x_train)\n",
    "x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "input_img_ae = Input(shape=img_shape)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(input_img_ae)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "encoded_ae = Flatten()(x) # Flatten before dense layer for latent space\n",
    "encoded_ae = Dense(latent_dim, activation='relu')(encoded_ae)\n",
    "\n",
    "encoder_ae = Model(input_img_ae, encoded_ae, name=\"encoder_ae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf6f31",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5bc995",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input_ae = Input(shape=(latent_dim,))\n",
    "x = Dense(7 * 7 * 64, activation='relu')(decoder_input_ae)\n",
    "x = Reshape((7, 7, 64))(x)\n",
    "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "decoded_ae = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "decoder_ae = Model(decoder_input_ae, decoded_ae, name=\"decoder_ae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bb1fce",
   "metadata": {},
   "source": [
    "#### Autoencoder Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4be631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_ae = Model(input_img_ae, decoder_ae(encoder_ae(input_img_ae)), name=\"autoencoder_ae\")\n",
    "autoencoder_ae.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "encoder_ae.summary()\n",
    "decoder_ae.summary()\n",
    "autoencoder_ae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08206e16",
   "metadata": {},
   "source": [
    "### 3.2. AE Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc3d506",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_ae = autoencoder_ae.fit(x_train, x_train,\n",
    "                                epochs=20, # Reduced for faster execution, ideally 50-100\n",
    "                                batch_size=128,\n",
    "                                shuffle=True,\n",
    "                                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76be41da-0d1f-4ac8-bb09-bf899ad570b0",
   "metadata": {},
   "source": [
    "### 3.3. AE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3646a0c9-6917-4f8c-9f33-5e61611a5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_ae.history['loss'], label='Training Loss')\n",
    "plt.plot(history_ae.history['val_loss'], label='Validation Loss')\n",
    "plt.title('AE Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0a687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original and reconstructed images\n",
    "def display_ae_results(original, reconstructed, n=10):\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Original\")\n",
    "\n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Reconstructed (AE)\")\n",
    "    plt.show()\n",
    "\n",
    "reconstructed_imgs_ae = autoencoder_ae.predict(x_test)\n",
    "display_ae_results(x_test, reconstructed_imgs_ae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56369fc0-c256-4f52-8507-4d770e585bd0",
   "metadata": {},
   "source": [
    "## 4. Variational Autoencoder (VAE)\n",
    "\n",
    "A VAE learns a probabilistic mapping to a latent space, allowing for generation of new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af0c8c9-e212-43bb-b42b-5a81cddf6d23",
   "metadata": {},
   "source": [
    "### 4.1. VAE Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc29cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function (reparameterization trick)\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33105580",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed9488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "input_img_vae = Input(shape=img_shape)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same', strides=2)(input_img_vae)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder_vae = Model(input_img_vae, [z_mean, z_log_var, z], name=\"encoder_vae\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59e9a5-3874-4518-9169-f602ab9d8117",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a1987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder (same architecture as AE's decoder for consistency)\n",
    "decoder_input_vae = Input(shape=(latent_dim,))\n",
    "x = Dense(7 * 7 * 64, activation='relu')(decoder_input_vae)\n",
    "x = Reshape((7, 7, 64))(x)\n",
    "x = Conv2DTranspose(64, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "x = Conv2DTranspose(32, (3, 3), activation='relu', padding='same', strides=2)(x)\n",
    "decoded_vae_output = Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "decoder_vae = Model(decoder_input_vae, decoded_vae_output, name=\"decoder_vae\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4fdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "outputs_vae = decoder_vae(encoder_vae(input_img_vae)[2])\n",
    "vae = Model(input_img_vae, outputs_vae, name='vae')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a097dacd-1e05-4c94-85cf-43ed0e310a3b",
   "metadata": {},
   "source": [
    "### VAE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24afe72-0313-48b1-9b0f-7a8501eff092",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Custom VAE class\n",
    "class VAE(Model):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        reconstruction_loss = tf.reduce_mean(\n",
    "            tf.keras.losses.binary_crossentropy(inputs, reconstructed)\n",
    "        ) * 28 * 28\n",
    "\n",
    "        # KL divergence\n",
    "        kl_loss = -0.5 * tf.reduce_mean(\n",
    "            1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "        )\n",
    "\n",
    "        self.add_loss(reconstruction_loss + kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "encoder_vae.summary()\n",
    "decoder_vae.summary()\n",
    "vae = VAE(encoder_vae, decoder_vae)\n",
    "vae.compile(optimizer='adam')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd02c54-4c86-4890-a5a6-feb555efbddb",
   "metadata": {},
   "source": [
    "### 4.2. VAE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0597440",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_vae = vae.fit(x_train, \n",
    "                      epochs=20, # Reduced for faster execution, ideally 50-100\n",
    "                      batch_size=128,\n",
    "                      validation_data=(x_test, None)) # No y_test needed as loss is internal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff206af-ad7e-4813-beeb-cca5b4fbb335",
   "metadata": {},
   "source": [
    "### 4.3. VAE Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12a863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history_vae.history['loss'], label='Training Loss')\n",
    "plt.plot(history_vae.history['val_loss'], label='Validation Loss')\n",
    "plt.title('VAE Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90be0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display original and reconstructed images\n",
    "def display_vae_reconstructed(original, vae_model, n=10):\n",
    "    reconstructed = vae_model.predict(original[:n])\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    for i in range(n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Original\")\n",
    "\n",
    "        # Display reconstruction\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if i == 0:\n",
    "            ax.set_title(\"Reconstructed (VAE)\")\n",
    "    plt.show()\n",
    "\n",
    "display_vae_reconstructed(x_test, vae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe2dde-0ba0-4596-893b-2ba21a8bc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new images from latent space\n",
    "def display_vae_generated(decoder, n=10):\n",
    "    random_latent_vectors = np.random.normal(size=(n, latent_dim))\n",
    "    generated_images = decoder.predict(random_latent_vectors)\n",
    "\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.suptitle(\"Generated Images (VAE)\")\n",
    "    plt.show()\n",
    "\n",
    "display_vae_generated(decoder_vae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a902bdc-3f79-4caa-9c59-5affb7576885",
   "metadata": {},
   "source": [
    "## 5. Generative Adversarial Network (GAN)\n",
    "A GAN consists of a generator and a discriminator that compete against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_latent_dim = 100 # Latent dimension for GAN generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379197a1-f7c6-42c0-902f-28a5f9333507",
   "metadata": {},
   "source": [
    "### 5.1. GAN Model Definition (DCGAN-like architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "def build_generator():\n",
    "    model = Sequential(name=\"generator\")\n",
    "    model.add(Dense(7 * 7 * 128, input_dim=gan_latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((7, 7, 128)))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Conv2D(1, (7,7), activation='sigmoid', padding='same')) # Output layer\n",
    "    return model\n",
    "\n",
    "generator_gan = build_generator()\n",
    "generator_gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832ea416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator\n",
    "def build_discriminator():\n",
    "    model = Sequential(name=\"discriminator\")\n",
    "    model.add(Conv2D(64, (3,3), strides=(2,2), padding='same', input_shape=img_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Conv2D(128, (3,3), strides=(2,2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "discriminator_gan = build_discriminator()\n",
    "discriminator_gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "discriminator_gan.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8121760d-f103-44aa-884d-79f9d268e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN (Combined Model)\n",
    "discriminator_gan.trainable = False # Discriminator is not trained when training GAN\n",
    "\n",
    "gan_input = Input(shape=(gan_latent_dim,))\n",
    "img = generator_gan(gan_input)\n",
    "gan_output = discriminator_gan(img)\n",
    "\n",
    "gan = Model(gan_input, gan_output, name=\"gan\")\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "gan.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2da41f",
   "metadata": {},
   "source": [
    "### 5.2. GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ebe6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_gan = 10000 # Reduced for faster execution, ideally 10000+\n",
    "_batch_size_gan = 64\n",
    "save_interval = 50 # Interval to save generated images\n",
    "\n",
    "# Rescale images to -1 to 1 for tanh activation if used, but sigmoid is fine with 0 to 1\n",
    "# x_train_gan = (x_train.astype(np.float32) - 127.5) / 127.5\n",
    "x_train_gan = x_train # Using 0-1 scaled images with sigmoid output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c1f7fa-f3af-44be-9061-f3d0a047777a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ground truths\n",
    "real = np.ones((_batch_size_gan, 1))\n",
    "fake = np.zeros((_batch_size_gan, 1))\n",
    "\n",
    "d_losses = []\n",
    "g_losses = []\n",
    "\n",
    "for epoch in range(epochs_gan):\n",
    "    # --------------------- #\n",
    "    #  Train Discriminator #\n",
    "    # --------------------- #\n",
    "    # Select a random batch of real images\n",
    "    idx = np.random.randint(0, x_train_gan.shape[0], _batch_size_gan)\n",
    "    real_imgs = x_train_gan[idx]\n",
    "\n",
    "    # Generate a batch of new images\n",
    "    noise = np.random.normal(0, 1, (_batch_size_gan, gan_latent_dim))\n",
    "    gen_imgs = generator_gan.predict(noise)\n",
    "\n",
    "    # Labels for real and fake images\n",
    "    real = np.ones((_batch_size_gan, 1))  # Label for real images (1)\n",
    "    fake = np.zeros((_batch_size_gan, 1))  # Label for fake images (0)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator_gan.train_on_batch(real_imgs, real)\n",
    "    d_loss_fake = discriminator_gan.train_on_batch(gen_imgs, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ----------------- #\n",
    "    #  Train Generator  #\n",
    "    # ----------------- #\n",
    "    noise = np.random.normal(0, 1, (_batch_size_gan, gan_latent_dim))\n",
    "    g_loss = gan.train_on_batch(noise, real)  # Train generator to fool discriminator\n",
    "\n",
    "    # Save losses for monitoring\n",
    "    d_losses.append(d_loss[0])\n",
    "    g_losses.append(g_loss)\n",
    "\n",
    "    # Logging the losses\n",
    "    print(f\"Epoch {epoch+1}/{epochs_gan}\")\n",
    "    print(f\"D loss: {d_loss[0]:.4f}, G loss: {g_loss:.4f}\")\n",
    "\n",
    "    # Check if losses are stuck or training is not progressing\n",
    "    if epoch > 1 and abs(d_losses[-1] - d_losses[-2]) < 1e-5 and abs(g_losses[-1] - g_losses[-2]) < 1e-5:\n",
    "        print(\"Training stuck, exiting early...\")\n",
    "        break\n",
    "\n",
    "    # Save generated images at intervals\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        r, c = 5, 5\n",
    "        noise_display = np.random.normal(0, 1, (r * c, gan_latent_dim))\n",
    "        gen_imgs_display = generator_gan.predict(noise_display)\n",
    "        gen_imgs_display = 0.5 * gen_imgs_display + 0.5  # Rescale to 0-1 if necessary\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs_display[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        plt.suptitle(f\"Generated Images at Epoch {epoch+1}\")\n",
    "        plt.show()\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c55460-dc4c-452a-a91a-8d2d0bd00e5d",
   "metadata": {},
   "source": [
    "### 5.3. GAN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c606f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(d_losses, label='Discriminator Loss')\n",
    "plt.plot(g_losses, label='Generator Loss')\n",
    "plt.title('GAN Training Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa72168",
   "metadata": {},
   "source": [
    "# Display some final generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f107538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_gan_generated(generator, n=10, latent_dim_val=gan_latent_dim):\n",
    "    noise = np.random.normal(0, 1, (n, latent_dim_val))\n",
    "    generated_images = generator.predict(noise)\n",
    "    plt.figure(figsize=(20, 2))\n",
    "    for i in range(n):\n",
    "        ax = plt.subplot(1, n, i + 1)\n",
    "        plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.suptitle(\"Final Generated Images (GAN)\")\n",
    "    plt.show()\n",
    "\n",
    "display_gan_generated(generator_gan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3edab-1220-4b65-84fd-d1a5a588bf05",
   "metadata": {},
   "source": [
    "## 6. Model Comparison and Conclusion\n",
    "\n",
    "Briefly compare the outputs and characteristics of AE, VAE, and GAN.\n",
    "\n",
    "- **Autoencoders (AE)** are good for dimensionality reduction and feature learning. Their reconstructions are typically faithful but not necessarily novel.\n",
    "- **Variational Autoencoders (VAE)** can generate new, plausible samples by sampling from the learned latent space. The generated images are often blurrier than GANs but VAEs provide a smooth latent space.\n",
    "- **Generative Adversarial Networks (GAN)** are known for producing sharp and realistic images. However, they can be harder to train (mode collapse, non-convergence).\n",
    "\n",
    "This notebook provided a basic implementation of these three models. Further improvements can be made by using more complex architectures, more extensive training, and hyperparameter tuning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
